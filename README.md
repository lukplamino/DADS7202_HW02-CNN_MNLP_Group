# DADS7202_Group Assignment 2 CNN (Group_MNLP)
> Objective: **`What do you use to train an image classiffier with our custom image dataset?`**
## ‚ú®Highlight
1.	Based on pre-trained model, the highest average accuracy rate is 85.37% of Xception model
2.	The best model after fine tuning pre-trained model is VGG16. The average accuracy on test data is at 91.66% rising from 84.63% (no tune)
3.	InceptionV3 and VGG16 show notable improvement on test accuracy at +8.71% and +7.04%, respectively, while ResNetV2 and Xception have less improvement at +2.59% and +2.22%, respectively
4.	Overall, the models have learned a distinctive of each banana type from banana fruit shape and shape of banana cut stalk


<!-- ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö ‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• insight ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà  3-5 bullets -->

## Table of Contents

 - [1. IntroductionüéØ](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#1-introduction)
 - [2. Dataüìë](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group#2-data)
 - [3. Network architectureüì¶](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group#3-network-architecture)
 - [4. TrainingüîÆ](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group#4-training)
 - [5. Resultsüìà](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group#5-results)
 - [6. Discussionüí≠](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#6-discussion)
 - [7. Conclusionüìù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#7-conclusion)
 - [8. Referencesüåê](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#8-references)
 - [Citing](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#citing)
 - [üë• Members, Percent Contribution and Responsibility](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#-members-percent-contribution-and-responsibility)
 - [üñáÔ∏èEnd Credit ](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#%EF%B8%8Fend-credit)



## 1. IntroductionüéØ 

**`multi-class image classification`**:

- This project aims to test **4 CNN pre-training models** (`VGG16`, `ResNet50V2`, `Xception`, `InceptionV3`) on the ImageNet dataset and fine-tune it to classify 3 types of bananas üçå (`Cultivated banana`, `Lady finger banana`, `Cavendish banana`) which is our custom image dataset that were never trained on. 
- Then, we will compare performance of **4 CNN pre-training models** without transfer learning and with transfer learning (Fine-tuning).
- Finally, we use [**`Grad-CAM`**](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#-visualizing-what-cnn-learned-with-grad-cam4) technique to debug the model and gain more insight into what a trained CNN did.

 
## 2. Dataüìë
There are many banana<sup>0</sup> varieties in Thailand and each one of them has different characteristics. Let‚Äôs find out interesting facts about different varieties of banana before training and finetuning models.

 üçå **1. Cultivated banana - ‡∏Å‡∏•‡πâ‡∏ß‡∏¢‡∏ô‡πâ‡∏≥‡∏ß‡πâ‡∏≤**
  
  The fruit looks a little bit angled with a thick skin and a sweet flavor.
  
  <img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Cultivated%20banana.png" style="width:120px;"/>
 
 üçå **2. Lady finger banana - ‡∏Å‡∏•‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡πá‡∏ö‡∏°‡∏∑‡∏≠‡∏ô‡∏≤‡∏á**
  
  Lady finger banana is one of the smallest bananas. Its skin is thick with a soft flesh inside.
  
   <img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Lady%20finger%20banana.png" style="width:120px;"/>
  
   
 üçå **3. Cavendish banana - ‡∏Å‡∏•‡πâ‡∏ß‡∏¢‡∏´‡∏≠‡∏°**
  
  The fruit is long with a thin skin. It offers a sweet flavor along with a uniquely pleasant smell.
  
  <img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Cavendish%20banana.jpg" style="width:120px;"/>

- Total 600 images (200 images per 1 class)

| Class | Name               | No. of image |
|:---:|---|---:|
|`0`      | Lady Finger Banana | 200          |
|`1`      | CavendishBanana    | 200          |
|`2`      | Cultivated Banana  | 200          |
|       | **Total**             | **600**          |

 
#### üìçData source: 
- We use [**Download All Images**](https://chrome.google.com/webstore/detail/download-all-images/ifipmflagepipjokmbdecpmjbibjnakm?hl=en) extension in chrome web store to collect set of images by searching keywords (3 types of banana) from **`google image searching`** 


#### üßπData preparation:
- Collecting set of images from the Internet source is a quick and simple method to gather a set of images. Some facts, meanwhile, are not entirely accurate or useful. As a result, we have to manually remove several unnecessary images from the collection, such as banana dessert, banana tree trunk, other banana pieces, and duplicate images. Additionally, because the keyword and banana type are inconsistent, we need to recheck the¬†banana type labels.

#### Data pre-processing
- The set of images were rescaled to 224x224 pixels and normalized by the ImageDataGenerator class with **`Pixel Standardization`** technique (zero mean and unit variance)

‚ùï Be careful: The different normalization techniques<sup>5</sup> effect the model's performance. (Loss and Accuracy).
- We use **`‚ûïData Augmentation`** technique to increase the diversity of our training set by applying **`ImageDataGenerator`** 
- We apply various changes to the initial data. For example, image rotation, zoom, shifting and fliping

```
        width_shift_range = 5.0,   
        height_shift_range = 5.0,              
        rotation_range=10,                               
        zoom_range=0.2,
        horizontal_flip=True,                 
        vertical_flip=True,
        validation_split=0.3
```
<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Data_Augmentation.png" style="width:500px;"/>


#### ‚úÇÔ∏èData splitting (train/val/test):
- `random_state` = 3
- `test_size` = 0.3
- `validation_split` = 0.3
- **`Train set`**: 294
- **`Validation set`**: 126
- **`Test set`**: 180

[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)

## 3. Network architectureüì¶

### Pre-training Models 
In this experiment, we have selected 4 Pre-training Models for fine-tuning with [**IMAGENET**](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/)<sup>6</sup> dataset as weight and complied with 

#### [Pre-training model Infomation](https://keras.io/api/applications/)<sup>7</sup>

<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/pre-training-models_info.png" style="width:700px;"/>

#### Network Architecture of Pre-training model 
- To compare Network architecture of Pre-training model **`without Fine-tuning`**  VS **`with Fine-tuning`**
- Remark: Based on our dataset and experiment scope
<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Network_Architechture.png" style="width:900px;"/>

#### Network Diagram of Pre-training model with Fine-tuning
<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Network_Diagram.png" style="width:700px;"/>

<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Network_Diagram_InceptionV3.png" style="width:700px;"/>

<!-- ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ (‡πÄ‡∏ä‡πà‡∏ô ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á layer, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô nodes, activation function, regularization) ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á network diagram ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÇ‡∏î‡∏¢‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏û‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏≠‡πà‡∏≤‡∏ô ‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÑ‡∏õ‡∏™‡∏£‡πâ‡∏≤‡∏á network ‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ) -->

[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)


## 4. TrainingüîÆ
Our training strategy is...

<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/training_part.png" style="width:650px;"/>

<!-- ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ train ‡πÅ‡∏•‡∏∞ validate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÜ ‡πÄ‡∏ä‡πà‡∏ô training strategy (‡πÄ‡∏ä‡πà‡∏ô single loss, compound loss, two-step training, end-to-end training), loss, optimizer (learning rate, momentum, etc), batch size,
epoch, ‡∏£‡∏∏‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô CPU ‡∏´‡∏£‡∏∑‡∏≠ GPU ‡∏´‡∏£‡∏∑‡∏≠ TPU ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ, ‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß ‡∏Ø‡∏•‡∏Ø -->


[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)

## 5. Resultsüìà
<!-- ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ mean¬±SD ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ initial random weights ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3-5 ‡∏£‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3-5 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏≤‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Å‡∏±‡∏ô, ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£ train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö train vs. validation, ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏î underfit ‡∏´‡∏£‡∏∑‡∏≠ overfit ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà, ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ evaluation metric ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô train/val/test sets ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ç‡∏≠‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤, ‡∏´‡∏≤‡∏Å‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏≤‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô ‡πÜ (‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô) ‡∏ö‡∏ô any standard benchmark dataset ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏∞‡∏¢‡∏¥‡πà‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏á‡∏≤‡∏ô‡∏î‡∏π‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâtrain ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ inference ‡∏ö‡∏ô‡∏ã‡∏µ‡∏û‡∏µ‡∏¢‡∏π‡πÅ‡∏•‡∏∞‡∏à‡∏µ‡∏û‡∏µ‡∏¢‡∏π ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏Ø‡∏•‡∏Ø 

‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö train vs. validation (‡πÄ‡∏ä‡πà‡∏ô loss, accuracy) ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö scale ‡∏Ñ‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏î‡∏π underfit / overfit ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢-->

### üìä Model Performance Comparison
From the experiment, We fine-tune each pre-training model with Hyperparameter more than 40-50 times to find the best model's performance with highest accuracy, less loss and not over-fit.

We pre-train the model with initial random weights in the first round and more 2 rounds without random seed to calculate mean¬±SD of accuracy and loss on test set as the average of the model performance
In each round, accuracy and loss of test sets are not significantly different. That proves the model is good fit.

<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Performance1.png" style="width:500px;"/>



#### Accuracy and Loss on Train vs Validation sets

<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Acc_Loss.png" style="width:650px;"/>

#### Accuracy on Test set
- To compare the highest accuracy on test set of each Pre-training models under the same conditions and the same seeds,
<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Performance2.png" style="width:500px;"/>

### ü™ü Evaluation metric on Test set

<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Evaluation-Matrix.png" style="width:600px;"/>

### ‚åõ Runtime Comparison (on Train set) 
Time per inference step is the average of epoch.
- **`GPU`**: Tesla T4
- **`Epoch`**: 30
<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/Runtime.png" style="width:550px;"/>

- **`InceptionV3`** pre-training model with fine-tuning spended the shortest time per epoch to train the model on test set.
- On the contrary, **`Xception`** pre-training model with fine-tuning spended the shortest time per epoch among these 4 models.



### üî¶ Visualizing what CNN learned with `Grad-Cam`<sup>4</sup>
- We use the gradient-weighted class activation mapping (**`Grad-Cam`**) technique on VGG16 pre-training model with fine-tuning to understand which parts of the image are most important for classification. 
> _"The Grad-CAM technique utilizes the gradients of the classification score with respect to the final convolutional feature map, to identify the parts of an input image that most impact the classification score. The places where this gradient is large are exactly the places where the final score depends most on the data."<sup> 9</sup>_

<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/GRAD_CAM.png" style="width:650px;"/>

- For **`Cavendish`** bananas (No.4,5) and **`Cultivated`** bananas (No.7,8), Clearly, the center of the banana fruit has the greatest impact on the classification.
- Contrarily, for all images that the model predicted **`Lady Finger`** bananas (No.1,2,6), the model focused on the cut stalk of the banana for distinctive features.
- Moreover, we can use the Grad-CAM heat-maps to give us clues into why the model had trouble making the correct classification. 
- Look at the pictures (no.3,9) with wrong prediction, we can see from the Grad-CAM that the model is emphasizing background or male flowers and having trouble finding banana fruit features.


[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)


## 6. Discussionüí≠
-	With ImageDataGenerator(), apart from shift, zoom, rotation and flip, we have tried rescaling with variety methods. Firstly, rescale = 1./255, it shows the worth accuracy rate at about 0.3611 as it just resize pictures by dividing 255. Next, featurewise_center and featurewise_std_normalization which set dataset mean to 0 and normalized over dataset; as a result, accuracy spike to 0.6278. Lastly, samplewise_center and samplewise_std_normalization is similar to the previous method but done over each input. All in all, we chose samplewise_center and samplewise_std_normalization as it shows the higher accuracy rate at 0.7778
-	In the beginning of work, we tried load data with flow_from_directory() that labels of picture will be one-hot encoded i.e. [1 0 0 0] for label number 1; consequently, a loss function must be ‚ÄúCategoricalCrossentropy‚Äù. However, we decided to load data with NumPy that labeled data from [0-2] i.e [1] for label number 1. As a result, ‚ÄúSparseCategoricalCrossentropy‚Äù is chosen to be the loss function
-	We experience running out of free-access GPU on GoogleColab. So, we try using other accounts, using local GPU on our own computers, and registered for ColabPro since CNN have noticeable difference run-time on CPU and GPU. Execution with CPU roughly takes 40 times of time consuming over GPU (200s vs 5s per epochs)
-	Among learning rate of 0.01, 0.001, 0.0001 and 0.00001, learning rate at 0.001 shows the best accuracy over test data set 
Note: result dementated below is tested with InceptionV3 model

<p align="center">
<img src="https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Images/accuracy_compare.png" style="width:250px;"/>
</p>

-	Another hyperparameter that affects model accuracy is batch_size, moreover, it varies among models. We use batch_size among 32, 64, and 128
-	We also struggle with random seed, setting fixed random seed at the top does not affect the following codes i.e. accuracy over test set changes as we repeat run. The solution of this is to set seed to all code that available


[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)

## 7. Conclusionüìù
-	We select 4 pre-trained models; namely, VGG16, Xception, ResNetV2, and InceptionV3
-	The best pre-trained model (No Fine-tune) among the selected models is Xception with average accuracy rate at 85.37% while after fine tuning it turns out that VGG16 becomes the best model with the highest accuracy on test set at 91.66% from 84.63%
-	After fine-tuning, all selected models show improvement on test accuracy average at 5.23% 
-	The overall run-time among the selected model is 5-6s per epochs under GPU usage
-	The most precise classification is ‚ÄúCavandish Banana‚Äù. From Grad-Cam, it shows that the model focuses on banana to determine whether it is Cavandish banana or Cultivated banana whereas Lady finger banana is focused at the cut stalk of the banana for distinctive features by the model

<!-- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏ô‡∏µ‡πâ ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (research question) ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å (objective) ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á -->
[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)

## 8. Referencesüåê
<!-- ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô), ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏°‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πÄ‡∏õ‡∏≠‡∏£‡πå, ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏´‡∏¢‡∏¥‡∏ö‡∏¢‡∏∑‡∏°‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å github ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô ‡πÜ -->

### Library
- **`Pandas`**
- **`Numpy`**
- **`Sklearn`**
- **`Keras`**
- **`Matplotlib`**
- **`Seaborn`**
<!-- This content will not appear in the rendered Markdown -->

### Version
```
Python 3.7.15 (default, Oct 12 2022, 19:14:55) 
[GCC 7.5.0]

NumPy 1.21.6

The scikit-learn version is 1.0.2
TensorFlow 2.9.2
tf.keras.backend.image_data_format() = channels_last
TensorFlow detected 1 GPU(s):
.... GPU No. 0: Name = /physical_device:GPU:0 , Type = GPU
```
<!-- This content will not appear in the rendered Markdown -->

### References
- <sup>0</sup>_-. (2019)._
[**‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏•‡πâ‡∏ß‡∏¢‡πÜ**](https://www.topspicks.tops.co.th/single-post/tidbits-about-bananas2019). Topspicks.
- <sup>1</sup>_-. (2022, Sep 8)._ [**tf.keras.preprocessing.image.ImageDataGenerator**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator). TensorFlow.
- <sup>2</sup>_Dustin. (2022, Oct 20)._ [**[Notebooks update] New GPU (T4s) options & more CPU RAM**](https://www.kaggle.com/discussions/product-feedback/361104?fbclid=IwAR2qbmFZTP6BbI7T-hHAAg8ByGiM9cZW_Ik6nHK7-WlRAu8UzoF0R2yCKZY). Kaggle.
- <sup>3</sup>_[fchollet](https://twitter.com/fchollet). (2020, May 12)._ [**Transfer learning & fine-tuning**](https://keras.io/guides/transfer_learning/). Keras.
- <sup>4</sup>_[fchollet](https://twitter.com/fchollet). (2021, March 7)._ [**Grad-CAM class activation visualization**](https://keras.io/examples/vision/grad_cam/). Keras.
- <sup>5</sup>_Jason Brownlee. (2019, Jul 5)._ [**How to Normalize, Center, and Standardize Image Pixels in Keras**](https://machinelearningmastery.com/how-to-normalize-center-and-standardize-images-with-the-imagedatagenerator-in-keras/). Machine Learning Mastery.
- <sup>6</sup>_Lang, Steven and Bravo-Marquez, Felipe and Beckham, Christopher and Hall, Mark and Frank, Eibe. (2019)._ [**IMAGENET 1000 Class List**](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/). Github.
- <sup>7</sup>[**Keras Applications**](https://keras.io/api/applications/). Keras.
- <sup>8</sup>[**Training strategy**](https://www.neuraldesigner.com/learning/tutorials/training-strategy). Neuraldesigner.
- <sup>9</sup>[**Grad-CAM Reveals the Why Behind Deep Learning Decisions**](https://www.mathworks.com/help/deeplearning/ug/gradcam-explains-why.html). Mathworks.



[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)

## Citing
[Bib.file](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/Citing_CNN_MNLP.bib)

```
@Misc{MNLP,
    AUTHOR          = {Navapol San. , Pakawut Kam. , Supisara Poo. , Kantima Tec.},
    TITLE           = {Model : CNN image classification model with finetuning on a custom dataset},
    YEAR            = {2022},
    howpublished    = "\url{https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group.git}"
}
```


[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)


## üë• Members, Percent Contribution and Responsibility 
|No  |ID                |Name                              |% Contribution |Responsibility                             |
|:---:|:---:             |---                               |:---:            |:---|
|1.  |**`6410422002`**  |[Navapol San.](https://www.kaggle.com/navapol)                      |   **`25%`**     |**`Collecting data (Cavendish banana)`**, **`Fine-tune Model (Xception)`**
|2.  |**`6410422003`**  |[Pakawut Kam.](https://www.kaggle.com/ppakawut)                     |   **`25%`**     |**`Collecting data (Lady finger banana)`**, **`Fine-tune Model (InceptionV3)`**  |
|3.  |**`6410422024`**  |[Supisara Poo.](https://www.kaggle.com/supisarapo)                     |   **`25%`**     |**`Collecting data (Cultivated banana)`**, **`Fine-tune Model (ResNet50V2)`**, **`Summary the Conclution`**    |
|4.  |**`6410422027`**  |[Kantima Tec.](https://www.kaggle.com/kantimatec)                     |   **`25%`**     |**`Fine-tune Model (VGG16)`**, **`Summary the report`**  |


[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)


## üñáÔ∏èEnd Credit 
This project is a part of **`DADS7202 Deep Learning`**

Term: 1 Year of education: 2022

üéìMaster of Science Program in **`Data Analytics and Data Science`** (DADS)

üè´National Institute of Development Administration (**`NIDA`**)



[üîù](https://github.com/lukplamino/DADS7202_HW02-CNN_MNLP_Group/blob/main/README.md#highlight)


----------------------------------------------
<!--
‚ùë Github link (public access) 1 ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ï‡πà‡∏≠ 1 ‡∏Å‡∏•‡∏∏‡πà‡∏° ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏á‡∏≤‡∏ô

‚ùë Highlight: ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏Æ‡πÑ‡∏•‡∏ó‡πå‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à bullet ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 3-5 bullets ‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö ‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• insight ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ

‚ùë Introduction: ‡∏ó‡∏≥ task ‡∏≠‡∏∞‡πÑ‡∏£ ‡πÄ‡∏ä‡πà‡∏ô binary classification, single-label multi-class classification, multi-label classification, regression, segmentation, etc.

‚ùë Data: 
- Data source, 
- EDA, 
- data preparation, 
- data pre-processing, 
- data post-processing, 
- data splitting (train/val/test) 
- ‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà balance ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™, 
- ‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ (‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏á‡∏≤‡∏ô classification)

‚ùë Network architecture: ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ (‡πÄ‡∏ä‡πà‡∏ô ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á layer, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô nodes, activation function, regularization) ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á network diagram ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÇ‡∏î‡∏¢‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏û‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏≠‡πà‡∏≤‡∏ô ‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÑ‡∏õ‡∏™‡∏£‡πâ‡∏≤‡∏á network ‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ)

‚ùë Training: ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ train ‡πÅ‡∏•‡∏∞ validate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÜ ‡πÄ‡∏ä‡πà‡∏ô training strategy (‡πÄ‡∏ä‡πà‡∏ô single loss, compound loss, two-step training, end-to-end training), loss, optimizer (learning rate, momentum, etc), batch size,
epoch, ‡∏£‡∏∏‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô CPU ‡∏´‡∏£‡∏∑‡∏≠ GPU ‡∏´‡∏£‡∏∑‡∏≠ TPU ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ, ‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß ‡∏Ø‡∏•‡∏Ø

‚ùë Results: ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ mean¬±SD ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ initial random weights ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3-5 ‡∏£‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3-5 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏≤‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Å‡∏±‡∏ô, ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£ train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö train vs. validation, ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏î underfit ‡∏´‡∏£‡∏∑‡∏≠ overfit ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà, ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ evaluation metric ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô train/val/test sets ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ç‡∏≠‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤, ‡∏´‡∏≤‡∏Å‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏≤‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô ‡πÜ (‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô) ‡∏ö‡∏ô any standard benchmark dataset ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏∞‡∏¢‡∏¥‡πà‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏á‡∏≤‡∏ô‡∏î‡∏π‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâtrain ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ inference ‡∏ö‡∏ô‡∏ã‡∏µ‡∏û‡∏µ‡∏¢‡∏π‡πÅ‡∏•‡∏∞‡∏à‡∏µ‡∏û‡∏µ‡∏¢‡∏π ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏Ø‡∏•‡∏Ø

o (Optional) Ablation study: ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡πà‡∏≠‡∏¢‡∏ã‡πâ‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏≠‡∏µ‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏ô‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡πÉ‡∏î‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô‡∏ï‡πà‡∏≠‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ô‡∏¥‡∏¢‡∏°‡∏ó‡∏≥ ablation study ‡πÇ‡∏î‡∏¢‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏•‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡πà‡∏≠‡∏¢‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏≠‡∏Å ‡πÅ‡∏•‡πâ‡∏ß train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏≠‡∏≠‡∏Å‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏ú‡∏•‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏¢‡πà‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£ ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÑ‡∏´‡∏ô‡∏î‡∏∂‡∏á‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ú‡∏•‡πÅ‡∏¢‡πà‡∏•‡∏á‡∏°‡∏≤‡∏Å‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ï‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏≠‡∏≤‡∏≠‡∏≠‡∏Å ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÑ‡∏´‡∏ô‡∏î‡∏∂‡∏á‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πá‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡∏á‡πÑ‡∏î‡πâ

‚ùë Discussion: ‡∏≠‡∏†‡∏¥‡∏õ‡∏£‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ê‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ú‡∏¥‡∏î‡∏Ñ‡∏≤‡∏î ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ê‡∏≤‡∏ô‡∏ö‡πâ‡∏≤‡∏á, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ß‡πà‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏Ñ‡∏≤‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏ô‡∏±‡πâ‡∏ô‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏≠‡∏∞‡πÑ‡∏£, ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà dataset ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏£‡∏ì‡∏µ imbalanced dataset) ‡∏Ñ‡∏ß‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡πà‡∏≤‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á dataset ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

‚ùë Conclusion: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏ô‡∏µ‡πâ ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (research question) ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å (objective) ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á

‚ùë References: ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô), ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏°‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πÄ‡∏õ‡∏≠‡∏£‡πå, ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏´‡∏¢‡∏¥‡∏ö‡∏¢‡∏∑‡∏°‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å github ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô ‡πÜ

‚ùë Citing: ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ô‡∏≠‡∏¢‡∏≤‡∏Å cite (‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á) ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠ dataset ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏≤ cite ‡πÄ‡∏£‡∏≤‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£ ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏ô‡∏¥‡∏¢‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á bibtex format ‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏†‡∏≤‡∏û

‚ùë ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏° ‡∏û‡∏£‡πâ‡∏≠‡∏° contribution percentage ‡∏Ç‡∏≠‡∏á‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏á‡∏≤‡∏ô ‡∏ó‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÄ‡∏ï‡πá‡∏°‡πÉ‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏ï‡πà‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏Å‡πá‡πÑ‡∏î‡πâ

‚ùë End credit: ‡∏Ç‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡πÄ‡∏•‡πá‡∏Å ‡πÜ ‡∏ô‡∏¥‡∏î‡∏ô‡∏∂‡∏á‡∏Ñ‡πà‡∏∞ ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ß‡πà‡∏≤ ‚Äú‡∏á‡∏≤‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á ...‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤... ...‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£... ...‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏´‡∏≤‡∏•‡∏±‡∏¢...‚Äù (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ì‡∏∞‡πÑ‡∏õ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß)

***
3. ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö train vs. validation (‡πÄ‡∏ä‡πà‡∏ô loss, accuracy) ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö scale ‡∏Ñ‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏î‡∏π underfit / overfit ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢
4. ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏î ‡πÜ ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏™‡∏°‡∏≠‡∏ß‡πà‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ö‡∏ô train set ‡∏´‡∏£‡∏∑‡∏≠ val set ‡∏´‡∏£‡∏∑‡∏≠ test set
5. ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏ß‡πâ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô ‚Äú‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á xxx‚Äù ‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£‡∏à‡∏≤‡∏Å‡πÑ‡∏´‡∏ô‡∏Å‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡∏ö‡πâ‡∏≤‡∏á‡∏ô‡∏≥‡∏°‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Å‡∏±‡∏ô
6. ‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÉ‡∏î ‡πÜ ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏ö‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏¢‡∏∏‡∏ï‡∏¥‡∏ò‡∏£‡∏£‡∏°‡∏ï‡πà‡∏≠‡∏Ñ‡∏π‡πà‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡∏≤‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö training time ‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏´‡∏ô‡∏°‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏±‡∏ô ‡∏Å‡πá‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏õ‡πá‡∏ô training time per one epoch (‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ ‡πÜ epoch), ‡∏´‡∏≤‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö inference time per one sample ‡∏Å‡πá‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏°‡∏≤‡∏à‡∏≤‡∏Å test samples ‡∏ä‡∏∏‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡∏ö‡∏ô CPU ‡∏´‡∏£‡∏∑‡∏≠ GPU ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô, ‡∏´‡∏≤‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡πà‡∏≤ loss ‡∏°‡∏≤‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏±‡∏ô ‡∏Å‡πá‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì loss ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô
7. ‡∏Å‡∏≤‡∏£‡∏≠‡∏†‡∏¥‡∏õ‡∏£‡∏≤‡∏¢‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡∏°‡∏¥‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô general conclusion ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡πÉ‡∏ô‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï ‡∏°‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏î ‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß
************** THE END **************-->
